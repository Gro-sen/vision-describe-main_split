import os
import glob
import pickle
import re
from pathlib import Path
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss

def smart_chunk_text(text, source_file, max_chars=400):
    """æ™ºèƒ½åˆ†å—æ–‡æœ¬ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
    chunks = []
    
    # æŒ‰æ ‡é¢˜åˆ†å‰²ï¼ˆ## æ ‡é¢˜ï¼‰
    title_sections = re.split(r'(?=\n## )', text.strip())
    
    for section in title_sections:
        if not section.strip():
            continue
        
        # æå–æ ‡é¢˜
        title_match = re.match(r'^(#+\s+.+?)\n', section)
        title = title_match.group(1) if title_match else "æ— æ ‡é¢˜"
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', section)
        
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            if not para.strip():
                continue
            
            para_length = len(para)
            
            # å¦‚æœæ®µè½æœ¬èº«å°±å¾ˆå¤§ï¼Œéœ€è¦å†åˆ†å‰²
            if para_length > max_chars:
                # æŒ‰å¥å­åˆ†å‰²
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]\s*', para)
                for sentence in sentences:
                    if not sentence.strip():
                        continue
                    
                    sent_length = len(sentence)
                    if current_length + sent_length <= max_chars:
                        current_chunk.append(sentence)
                        current_length += sent_length
                    else:
                        # ä¿å­˜å½“å‰å—
                        if current_chunk:
                            chunk_text = 'ã€‚'.join(current_chunk) + 'ã€‚'
                            chunks.append({
                                'text': chunk_text,
                                'source': source_file,
                                'type': 'paragraph',
                                'title': title
                            })
                        
                        # å¼€å§‹æ–°å—
                        current_chunk = [sentence]
                        current_length = sent_length
            else:
                # æ®µè½é€‚åˆå½“å‰å—
                if current_length + para_length <= max_chars:
                    current_chunk.append(para)
                    current_length += para_length
                else:
                    # ä¿å­˜å½“å‰å—
                    if current_chunk:
                        chunk_text = '\n\n'.join(current_chunk)
                        chunks.append({
                            'text': chunk_text,
                            'source': source_file,
                            'type': 'paragraph_group',
                            'title': title
                        })
                    
                    # å¼€å§‹æ–°å—
                    current_chunk = [para]
                    current_length = para_length
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunk_text = '\n\n'.join(current_chunk)
            chunks.append({
                'text': chunk_text,
                'source': source_file,
                'type': 'paragraph_group',
                'title': title
            })
    
    # å¦‚æœæ²¡æœ‰åˆ†å—ï¼Œåˆ™æŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
    if not chunks:
        for i in range(0, len(text), max_chars):
            chunk_text = text[i:i+max_chars]
            chunks.append({
                'text': chunk_text,
                'source': source_file,
                'type': 'fixed_length',
                'title': 'æœªåˆ†å—å†…å®¹'
            })
    
    return chunks

def build_index(data_dir='kb/source', 
                index_path='kb/index/faiss_bge.index',
                meta_path='kb/index/docs_bge.pkl',
                model_name='BAAI/bge-small-zh-v1.5'):
    """æ„å»ºçŸ¥è¯†åº“ç´¢å¼•"""
    
    print("ğŸ”¨ å¼€å§‹æ„å»ºçŸ¥è¯†åº“ç´¢å¼•...")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(index_path) or '.', exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
    model = SentenceTransformer(model_name)
    dim = model.get_sentence_embedding_dimension()
    print(f"   æ¨¡å‹ç»´åº¦: {dim}")
    
    # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
    files = glob.glob(os.path.join(data_dir, '*.md'))
    
    if not files:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°çŸ¥è¯†åº“æ–‡ä»¶")
        return {'status': 'error', 'message': 'No source files found'}
    
    print(f"ğŸ“š æ‰¾åˆ° {len(files)} ä¸ªçŸ¥è¯†åº“æ–‡ä»¶")
    
    # åˆ†å—å¤„ç†
    all_chunks = []
    
    for filepath in files:
        filename = Path(filepath).name
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if not content.strip():
                continue
            
            # æ™ºèƒ½åˆ†å—
            chunks = smart_chunk_text(content, filename, max_chars=500)
            
            print(f"  {filename}: {len(chunks)} ä¸ªå—")
            
            all_chunks.extend(chunks)
            
        except Exception as e:
            print(f"  å¤„ç†æ–‡ä»¶ {filename} å¤±è´¥: {e}")
    
    if not all_chunks:
        print("âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„æ–‡æ¡£å—")
        return {'status': 'error', 'message': 'No chunks generated'}
    
    print(f"ğŸ“Š æ€»å…±ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    # å‡†å¤‡æ–‡æœ¬ç”¨äºç¼–ç 
    texts = [chunk['text'] for chunk in all_chunks]
    
    # åˆ›å»ºå†…ç§¯ç´¢å¼•ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    index = faiss.IndexFlatIP(dim)
    
    # åˆ†æ‰¹å¤„ç†å‘é‡åŒ–
    batch_size = 32
    print("âš¡ ç”Ÿæˆå‘é‡åµŒå…¥...")
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i+batch_size]
        
        # ç¼–ç å¹¶å½’ä¸€åŒ–
        embeddings = model.encode(
            batch_texts,
            convert_to_numpy=True,
            normalize_embeddings=True,  # å…³é”®ï¼šå½’ä¸€åŒ–å‘é‡
            show_progress_bar=False
        ).astype('float32')
        
        index.add(embeddings)
        
        progress = min(i + batch_size, len(texts)) / len(texts) * 100
        print(f"  è¿›åº¦: {progress:.1f}%", end='\r')
    
    print(f"\nâœ… å‘é‡åµŒå…¥å®Œæˆ")
    
    # ä¿å­˜ç´¢å¼•
    faiss.write_index(index, index_path)
    print(f"ğŸ’¾ ç´¢å¼•å·²ä¿å­˜: {index_path}")
    
    # ä¿å­˜å…ƒæ•°æ®
    with open(meta_path, 'wb') as f:
        pickle.dump(all_chunks, f)
    print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {meta_path}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    chunk_types = {}
    for chunk in all_chunks:
        chunk_type = chunk.get('type', 'unknown')
        chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
    
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»æ–‡æ¡£å—: {len(all_chunks)}")
    print(f"  ç´¢å¼•ç»´åº¦: {dim}")
    print(f"  ç´¢å¼•ç±»å‹: å†…ç§¯ç´¢å¼• (ä½™å¼¦ç›¸ä¼¼åº¦)")
    print(f"  å—ç±»å‹åˆ†å¸ƒ:")
    for t, c in chunk_types.items():
        print(f"    {t}: {c}")
    
    return {
        'status': 'success',
        'chunks_count': len(all_chunks),
        'dimension': dim,
        'index_path': index_path,
        'meta_path': meta_path,
        'model': model_name
    }

def rebuild_index():
    """é‡å»ºç´¢å¼•ï¼ˆä¸»å…¥å£å‡½æ•°ï¼‰"""
    print("="*60)
    print("çŸ¥è¯†åº“ç´¢å¼•é‡å»ºå·¥å…·")
    print("="*60)
    
    result = build_index()
    
    if result['status'] == 'success':
        print(f"\nâœ… ç´¢å¼•é‡å»ºæˆåŠŸï¼")
        print(f"   æ–‡æ¡£å—æ•°é‡: {result['chunks_count']}")
        print(f"   æ¨¡å‹: {result['model']}")
        print(f"   ç´¢å¼•æ–‡ä»¶: {result['index_path']}")
        print(f"\nâš ï¸  è¯·é‡å¯åº”ç”¨ä»¥ä½¿ç”¨æ–°ç´¢å¼•")
    else:
        print(f"\nâŒ ç´¢å¼•é‡å»ºå¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
    
    return result

if __name__ == "__main__":
    rebuild_index()